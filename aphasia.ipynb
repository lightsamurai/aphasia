{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APHASIC PATIENTS' LINGUISTIC PRODUCTION\n",
    "# BY COMPUTATIONAL MEANS WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim of this work is to automatize the diagnosis of aphasic patients productions using the word2vec algorithm.\n",
    "Word2vec uses a neural network model to learn word associations from a large corpus of text.\n",
    "\n",
    "We collected our input data into 2 lists of word pairs from the Aphasia Bank (https://aphasia.talkbank.org/).\n",
    "One list is made of 170 EN target/response word pairs.\n",
    "One list is made of 290 IT target/response word pairs.\n",
    "\n",
    "We want to compare the cosine similarity between the target/response pairs in EN.\n",
    "We used the pre-trained 'word2vec-google-news-300' vectors to run the cosine similarity task.\n",
    "\n",
    "We compute the cosine similarity between target and response words using the built-in wv.similarity task of Word2Vec, taking as input our word pairs\n",
    "and using their vectorized form as vectorized in the Google model.\n",
    "\n",
    "For the cosine similarity task for the target/response pairs in IT we trained a model with the SkipGram algorithm of Word2Vec basing on 10 million word from Wikipedia using plainstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the excel file with pair target/response\n",
    "# pandas***\n",
    "\n",
    "from csv import reader\n",
    "import csv\n",
    "\n",
    "with open(\"/Users/silviafabbi/Desktop/Pairs_EN.csv\", \"r\") as pairs:\n",
    "    \n",
    "    csv_reader = reader(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ball   bound\n",
      "0      ball    bask\n",
      "1    window    womb\n",
      "2    broken  bottom\n",
      "3      word   money\n",
      "4      rain     run\n",
      "..      ...     ...\n",
      "97      age    days\n",
      "98   to say  to see\n",
      "99     foot    head\n",
      "100   woman     man\n",
      "101     put    foot\n",
      "\n",
      "[102 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "# Create a dataframe from csv\n",
    "df = pd.read_csv(\"/Users/silviafabbi/Desktop/Pairs_EN.csv\", \"r\", delimiter=',', engine='python')\n",
    "\n",
    "# User list comprehension to create a list of lists from Dataframe rows\n",
    "list_of_rows = [list(row) for row in df.values]\n",
    "\n",
    "print(df)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL 1 - EN - vectors from GoogleNews\n",
    "\n",
    "# We use gensim to import a word2vec model pretrained on google news \n",
    "# We load the pretrained model of the type #gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "# using the gensim stanrad method .load()\n",
    "# This type of pretrained model cannot be refined with additional data\n",
    "# but has the advantage of saving RAM by dealing with huge quantity of data\n",
    "# The 'word2vec-google-news-300' are pre-trained vectors trained on Google News dataset (about 100 billion words)\n",
    "#Â The model contains 300-dimensional vectors for 3 million words and phrases\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')\n",
    "type(wv) # gensim.models.keyedvectors.Word2VecKeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ball'\t'bound'\t0.0339\n",
      "'ball'\t'bask'\t0.0549\n",
      "'window'\t'womb'\t0.1947\n",
      "'broken'\t'bottom'\t0.1166\n",
      "'word'\t'money'\t0.2119\n",
      "'rain'\t'run'\t0.1114\n",
      "'dog'\t'guy'\t0.2742\n",
      "'barking'\t'biting'\t0.3858\n",
      "'cat'\t'god'\t0.1099\n",
      "'girl'\t'guy'\t0.3644\n",
      "'tree'\t'train'\t0.1557\n",
      "'ball'\t'bald'\t0.1118\n",
      "'ball'\t'barn'\t0.0713\n",
      "'ball'\t'banks'\t0.0107\n",
      "'glass'\t'grass'\t0.1117\n",
      "'woman'\t'man'\t0.7664\n",
      "'somewhere'\t'someone'\t0.4419\n",
      "'wear'\t'words'\t0.0561\n",
      "'shoe'\t'scene'\t0.0908\n",
      "'boy'\t'man'\t0.6825\n",
      "'mother'\t'wife'\t0.7551\n",
      "'school'\t'cool'\t0.1263\n",
      "'girl'\t'earl'\t0.2010\n",
      "'slipper'\t'sipper'\t0.1738\n",
      "'lamp'\t'lap'\t0.1483\n",
      "'umbrella'\t'ball'\t0.0570\n",
      "'short'\t'sort'\t0.1629\n",
      "'boy'\t'boil'\t0.1139\n",
      "'woman'\t'man'\t0.7664\n",
      "'fourth'\t'force'\t0.1147\n",
      "'door'\t'window'\t0.6213\n",
      "'fireman'\t'policeman'\t0.5428\n",
      "'glass'\t'gas'\t0.0585\n",
      "'soaking'\t'stoking'\t0.2246\n",
      "'ladder'\t'window'\t0.3029\n",
      "'kick'\t'cook'\t0.0898\n",
      "'window'\t'door'\t0.6213\n",
      "'umbrella'\t'comb'\t0.0946\n",
      "'umbrella'\t'bread'\t0.0462\n",
      "'umbrella'\t'read'\t-0.0125\n",
      "'catch'\t'kitchen'\t0.0604\n",
      "'ball'\t'bell'\t0.0863\n",
      "'soccer'\t'sock'\t0.1546\n",
      "'mother'\t'daughter'\t0.8706\n",
      "'lamp'\t'lights'\t0.4994\n",
      "'give'\t'gay'\t0.0486\n",
      "'rescue'\t'like'\t0.0662\n",
      "'dog'\t'door'\t0.1997\n",
      "'bark'\t'talk'\t0.1986\n",
      "'kick'\t'cook'\t0.0898\n",
      "'show'\t'go'\t0.1650\n",
      "'kick'\t'hit'\t0.2502\n",
      "'both'\t'bow'\t0.0941\n",
      "'cat'\t'hat'\t0.1706\n",
      "'dog'\t'boy'\t0.3522\n",
      "'cat'\t'girl'\t0.3039\n",
      "'reach'\t'crawl'\t0.2543\n",
      "'there'\t'hair'\t0.0724\n",
      "'ride'\t'drive'\t0.3714\n",
      "'quite'\t'white'\t0.0891\n",
      "'girl'\t'boy'\t0.8543\n",
      "'grow'\t'goes'\t0.1579\n",
      "'page'\t'bar'\t0.1206\n",
      "'umbrella'\t'black'\t0.1056\n",
      "'naughty'\t'nasty'\t0.4207\n",
      "'walk'\t'speak'\t0.2858\n",
      "'girl'\t'woman'\t0.7495\n",
      "'ride'\t'run'\t0.3192\n",
      "'cat'\t'hat'\t0.1706\n",
      "'branch'\t'window'\t0.2317\n",
      "'father'\t'mother'\t0.7901\n",
      "'tree'\t'try'\t0.0694\n",
      "'tree'\t'far'\t0.0678\n",
      "'dog'\t'fog'\t0.0666\n",
      "'ladder'\t'water'\t0.1233\n",
      "'truck'\t'bottle'\t0.1837\n",
      "'truck'\t'home'\t0.2014\n",
      "'fire'\t'ocean'\t0.0819\n",
      "'tree'\t'hospital'\t0.0841\n",
      "'bird'\t'song'\t0.1436\n",
      "'maid'\t'main'\t0.0134\n",
      "'woman'\t'man'\t0.7664\n",
      "'that'\t'dat'\t0.1025\n",
      "'foot'\t'pit'\t0.0751\n",
      "'slipper'\t'pitcher'\t0.0831\n",
      "'put'\t'pit'\t0.0629\n",
      "'sisters'\t'brothers'\t0.6871\n",
      "'elate'\t'delete'\t0.1346\n",
      "'boy'\t'man'\t0.6825\n",
      "'hands'\t'heads'\t0.3448\n",
      "'cry'\t'try'\t0.1564\n",
      "'wheels'\t'feels'\t0.1272\n",
      "'ladder'\t'letter'\t-0.0459\n",
      "'age'\t'days'\t0.2333\n",
      "'say'\t'see'\t0.4852\n",
      "'foot'\t'head'\t0.0673\n",
      "'woman'\t'man'\t0.7664\n",
      "'put'\t'foot'\t0.1940\n"
     ]
    }
   ],
   "source": [
    "# Now we compare the similarity of the target/response word pairs\n",
    "# using wv = api.load('word2vec-google-news-300') as a pretrained model\n",
    "# pairs - is a list of the tuples made by target/response word\n",
    "# type(pairs) = _io.TextIOWrapper***\n",
    "\n",
    "pairs = [\n",
    "    ('ball', 'bound'),\n",
    "    ('ball', 'bask'),\n",
    "    ('window', 'womb'),\n",
    "    ('broken', 'bottom'),\n",
    "    ('word', 'money'),\n",
    "    ('rain', 'run'),\n",
    "    ('dog', 'guy'),\n",
    "    ('barking', 'biting'),\n",
    "    ('cat', 'god'),\n",
    "    ('girl', 'guy'),\n",
    "    ('tree', 'train'),\n",
    "    ('ball', 'bald'),\n",
    "    ('ball', 'barn'),\n",
    "    ('ball', 'banks'),\n",
    "    ('glass', 'grass'),\n",
    "    ('woman', 'man'),\n",
    "    ('somewhere', 'someone'),\n",
    "    ('wear', 'words'),\n",
    "    ('shoe', 'scene'),\n",
    "    ('boy', 'man'),\n",
    "    ('mother', 'wife'),\n",
    "    ('school', 'cool'),\n",
    "    ('girl', 'earl'),\n",
    "    ('slipper', 'sipper'),\n",
    "    ('lamp', 'lap'),\n",
    "    ('umbrella', 'ball'),\n",
    "    ('short', 'sort'),\n",
    "    ('boy', 'boil'),\n",
    "    ('woman', 'man'),\n",
    "    ('fourth', 'force'),\n",
    "    ('door', 'window'),\n",
    "    ('fireman', 'policeman'),\n",
    "    ('glass', 'gas'),\n",
    "    ('soaking', 'stoking'),\n",
    "    ('ladder', 'window'),\n",
    "    ('kick', 'cook'),\n",
    "    ('window', 'door'),\n",
    "    ('umbrella', 'comb'),\n",
    "    ('umbrella', 'bread'),\n",
    "    ('umbrella', 'read'),\n",
    "    ('catch', 'kitchen'),\n",
    "    ('ball', 'bell'),\n",
    "    ('soccer', 'sock'),\n",
    "    ('mother', 'daughter'),\n",
    "    ('lamp', 'lights'),\n",
    "    ('give', 'gay'),\n",
    "    ('rescue', 'like'),\n",
    "    ('dog', 'door'),\n",
    "    ('bark', 'talk'),\n",
    "    ('kick', 'cook'),\n",
    "    ('show', 'go'),\n",
    "    ('kick', 'hit'),\n",
    "    ('both', 'bow'),\n",
    "    ('cat', 'hat'),\n",
    "    ('dog', 'boy'),\n",
    "    ('cat', 'girl'),\n",
    "    ('reach', 'crawl'),\n",
    "    ('there', 'hair'),\n",
    "    ('ride', 'drive'),\n",
    "    ('quite', 'white'),\n",
    "    ('girl', 'boy'),\n",
    "    ('grow', 'goes'),\n",
    "    ('page', 'bar'),\n",
    "    ('umbrella', 'black'),\n",
    "    ('naughty', 'nasty'),\n",
    "    ('walk', 'speak'),\n",
    "    ('girl', 'woman'),\n",
    "    ('ride', 'run'),\n",
    "    ('cat', 'hat'),\n",
    "    ('branch', 'window'),\n",
    "    ('father', 'mother'),\n",
    "    ('tree', 'try'),\n",
    "    ('tree', 'far'),\n",
    "    ('dog', 'fog'),\n",
    "    ('ladder', 'water'),\n",
    "    ('truck', 'bottle'),\n",
    "    ('truck', 'home'),\n",
    "    ('fire', 'ocean'),\n",
    "    ('tree', 'hospital'),\n",
    "    ('bird', 'song'),\n",
    "    ('maid', 'main'),\n",
    "    ('woman', 'man'),\n",
    "    ('that', 'dat'),\n",
    "    ('foot', 'pit'),\n",
    "    ('slipper', 'pitcher'),\n",
    "    ('put', 'pit'),\n",
    "    ('sisters', 'brothers'),\n",
    "    ('elate', 'delete'),\n",
    "    ('boy', 'man'),\n",
    "    ('hands', 'heads'),\n",
    "    ('cry', 'try'),\n",
    "    ('wheels', 'feels'),\n",
    "    ('ladder', 'letter'),\n",
    "    ('age', 'days'),\n",
    "    ('say', 'see'),\n",
    "    ('foot', 'head'),\n",
    "    ('woman', 'man'),\n",
    "    ('put', 'foot'),   \n",
    "]\n",
    "\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.4f' % (w1, w2, wv.similarity(w1, w2)))\n",
    "    \n",
    "# meaning last line of code:    \n",
    "# \"%r\\t%r\\t\" sono caratteri \"jolly\" che vengono sostituiti col contenuto nella n-upla passata per mezzo di \"%\"\n",
    "# In questo caso hai una stringa \"%r\" separata da una tabulazione \"\\t\" seguita da un altro \"%r\" e relativa tabulazione \n",
    "# \"% .2f\" chiude con un float a 4 decimali\n",
    "# Python sa che deve sostituire, nell'ordine specificato,\n",
    "# quelle sequenze con il contenuto delle variabili passate per mezzo di \"%\" prima di stamparle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('balls', 0.6992625594139099),\n",
       " ('upfield', 0.6896207928657532),\n",
       " ('downfield', 0.6390728950500488),\n",
       " ('dribbler', 0.6218727827072144),\n",
       " ('balll', 0.6199932098388672),\n",
       " ('dribble', 0.616877555847168),\n",
       " ('ball_squirted', 0.6110137701034546),\n",
       " ('leftfooted', 0.6020259857177734),\n",
       " ('puck', 0.5981724262237549),\n",
       " ('mishit', 0.5948782563209534),\n",
       " ('lofted', 0.5933606028556824),\n",
       " ('theball', 0.5924203395843506),\n",
       " ('bobbling', 0.5848650336265564),\n",
       " ('dinked', 0.5820186138153076),\n",
       " ('dribbles', 0.5811805725097656),\n",
       " ('beautifully_flighted', 0.5757741928100586),\n",
       " ('mistimes', 0.5747321844100952),\n",
       " ('onsides', 0.5730898380279541),\n",
       " ('perfectly_flighted', 0.5724466443061829),\n",
       " ('deadball', 0.5708563923835754)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"ball\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revel: 0.4761\n"
     ]
    }
   ],
   "source": [
    "result = wv.most_similar(positive=['ball', 'bask'], negative=['barn'])\n",
    "\n",
    "most_similar_key, similarity = result[0]  # look at the first match\n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_io.TextIOWrapper"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panther: 0.5573\n"
     ]
    }
   ],
   "source": [
    "# MOST SIMILAR FOR ENGLISH\n",
    "\n",
    "result = wv.most_similar(positive=['tiger', 'puma'], negative=['lion'])\n",
    "\n",
    "most_similar_key, similarity = result[0]  # look at the first match\n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tigers: 0.5206\n"
     ]
    }
   ],
   "source": [
    "result = wv.most_similar(positive=['tiger', 'puma'], negative=['lion'])\n",
    "\n",
    "most_similar_key, similarity = result[1]  # look at the second match\n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaguars: 0.4805\n"
     ]
    }
   ],
   "source": [
    "result = wv.most_similar(positive=['tiger', 'puma'], negative=['lion'])\n",
    "\n",
    "most_similar_key, similarity = result[2]  # look at the third match\n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try another model basing on wikipedia extracted with GloVe (an alternative to Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "# type(model) = gensim.models.keyedvectors.Word2VecKeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ball'\t'bound'\t0.3754\n",
      "'ball'\t'bask'\t0.0131\n",
      "'window'\t'womb'\t0.2678\n",
      "'broken'\t'bottom'\t0.4862\n",
      "'word'\t'money'\t0.4260\n",
      "'rain'\t'run'\t0.3650\n",
      "'dog'\t'guy'\t0.5169\n",
      "'barking'\t'biting'\t0.3643\n",
      "'cat'\t'god'\t0.3051\n",
      "'girl'\t'guy'\t0.5142\n",
      "'tree'\t'train'\t0.2779\n",
      "'ball'\t'bald'\t0.2374\n",
      "'ball'\t'barn'\t0.2118\n",
      "'ball'\t'banks'\t0.2028\n",
      "'glass'\t'grass'\t0.3207\n",
      "'woman'\t'man'\t0.8323\n",
      "'somewhere'\t'someone'\t0.6049\n",
      "'wear'\t'words'\t0.2899\n",
      "'shoe'\t'scene'\t0.2811\n",
      "'boy'\t'man'\t0.7915\n"
     ]
    }
   ],
   "source": [
    "# let's compare the similarity of the word pairs using\n",
    "# model = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "for w1, w2 in pairs[:20]:\n",
    "    print('%r\\t%r\\t%.4f' % (w1, w2, model.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOST similar for Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vehicle', 0.8630838394165039),\n",
       " ('truck', 0.8597878217697144),\n",
       " ('cars', 0.837166965007782),\n",
       " ('driver', 0.8185911178588867),\n",
       " ('driving', 0.7812635898590088),\n",
       " ('motorcycle', 0.7553156614303589),\n",
       " ('vehicles', 0.7462256550788879),\n",
       " ('parked', 0.7459464073181152),\n",
       " ('bus', 0.737270712852478),\n",
       " ('taxi', 0.7155268788337708)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 3 - IT - vectors from a model from plainstream/wikipedia\n",
    "# model ITA from Wikipedia - 10 mln words\n",
    "# \n",
    "# After training a w2v model in ITA, I saved it as a .model file\n",
    "# The model was trained using the Skip Gram algorithm of Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import plainstream\n",
    "import gensim\n",
    "# We are introducing the time module: it has many uses, but here we are just using the .time() method\n",
    "# to measure the execution time of a process\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "# Here we are asking plainstream to give us a certain amount of words (10 milion in this case).\n",
    "# NB: a plainstream.get_text() obejct is a generator, which is empty after one use\n",
    "# Generator functions allow you to declare a function that behaves like an iterator\n",
    "# i.e. it can be used in a for loop\n",
    "some_wiki = plainstream.get_text(\"it\", max_words=10000000, tokenize=True)\n",
    "some_text = []\n",
    "\n",
    "# we want to make sure that every word is lower case. Because some_wiki generates lists\n",
    "# of lists of tokens (i.e: tokenized sentences) we need to nest a couple of for loops in order to \n",
    "# reach the strings that we want to manipulate\n",
    "\n",
    "for tokens_list in some_wiki:\n",
    "    temp = []\n",
    "    for word in tokens_list:\n",
    "        temp.append(word.lower())\n",
    "    some_text.append(temp)\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_wiki # <generator object get_text at 0x7fce21e0ac10>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "# this is where we train the model. We are using a couple of parameters here, but the most\n",
    "# relevant is \"sg\", which means that we are using the skipgram algorithm\n",
    "model_ita = gensim.models.word2vec.Word2Vec(sentences=some_text, size=300, min_count=4, sg=1)\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ita.save(\"/Users/silviafabbi/Desktop/ord2vec_10mil_wiki.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the trained model\n",
    "\n",
    "model_ita = KeyedVectors.load(\"/Users/silviafabbi/Desktop/ord2vec_10mil_wiki.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nuca', 0.7739449739456177),\n",
       " ('schiena', 0.7475646734237671),\n",
       " ('caviglia', 0.7293763160705566),\n",
       " ('mento', 0.7261757850646973),\n",
       " ('fratturata', 0.7234964370727539),\n",
       " ('pistola', 0.7210250496864319),\n",
       " ('afferra', 0.7146195769309998),\n",
       " ('fanciulla', 0.7145273089408875),\n",
       " (\"sull'ala\", 0.7128865718841553),\n",
       " ('porge', 0.7128502130508423)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ita.wv.most_similar(\"gamba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'auto'\t'tetto'\t0.32\n",
      "'cane'\t'freccia'\t0.52\n",
      "'scala'\t'piede'\t0.18\n"
     ]
    }
   ],
   "source": [
    "coppie = [\n",
    "    ('auto', 'tetto'), \n",
    "    ('cane', 'freccia'),   \n",
    "    ('scala', 'piede'),\n",
    "]\n",
    "for w1, w2 in coppie:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, model_ita.wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now compare the cosine similarity and Spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.7706852377098591, pvalue=1.5603042276140894e-197)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import stats\n",
    "\n",
    "with open(\"/Users/silviafabbi/Desktop/Lab_2_files/word2idx_v2.json\") as in_file:   # open the file that maps words to their own index in the matrix\n",
    "    word2idx = json.load(in_file)\n",
    "idx2word = {v: k for k, v in word2idx.items()}  # build the reversed dictionary: from indeces to words\n",
    "\n",
    "matrix = numpy.load(\"/Users/silviafabbi/Desktop/Lab_2_files/word2vec_vectors_lab_v2.npz\")[\"arr_0\"]\n",
    "human_relatedness = []  # Prepare a list for relatedness values assigned by human annotators for a word pair\n",
    "word2vec_relatedness = []  # Prepare a list for cosine similarities between word pairs using word embeddings \n",
    "\n",
    "men = open(\"/Users/silviafabbi/Desktop/Lab_2_files/MEN_lab_2.txt\")  # Open the test set file (MEN dataset)\n",
    "for line in men.readlines():  # iterate over the dataset file line by line \n",
    "    word1 = line.split()[0]  # split (no argument=whitespace) and take the first word\n",
    "    word2 = line.split()[1]  # split and take the second word\n",
    "    h_r = line.split()[2]  # split and take the third value, which is the relatedness assigned by humans to the word pair [word1, word2]\n",
    "    human_relatedness.append(float(h_r))  # add relatedness assigned by humans to the human_relatedness list. float() converts a string into a number. N.B.: when you read a file, everything is considered to be a string\n",
    "    word_embedding_1 = matrix[word2idx[word1]].reshape(1, -1)  # take the word embedding of word1. reshape(1,-1) turns a vector into a matrix of one single row. This is required to run the cosine_similarity function\n",
    "    word_embedding_2 = matrix[word2idx[word2]].reshape(1, -1)  # take the word embedding of word2. reshape(1,-1) turns a vector into a matrix of one single row. This is required to run the cosine_similarity function\n",
    "    cos_sim = cosine_similarity(word_embedding_1, word_embedding_2)[0]  # compute the cosine similarity between the two words. [0] is extract the value of the cosine similarity out of a matrix with a single element\n",
    "    word2vec_relatedness.append(float(cos_sim))  # add this cosime similarity value to the word2vec_relatedness list\n",
    "\n",
    "\n",
    "    \n",
    "print(stats.spearmanr(human_relatedness, word2vec_relatedness))  # compute the Spearman's r coefficient between the relatedness values assigned by human annotators and the cosine similarity between word embeddings\n",
    "# The first value corresponds to the correlation coefficient and the second value to the p-value (you will see more on this kind of statistical analyses in the Research Design cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1 is shade\n",
      "Word 2 is whiskers\n",
      "Human relatedness is 1\n"
     ]
    }
   ],
   "source": [
    "# how can I cycle through the whole \n",
    "\n",
    "men_raw = open(\"/Users/silviafabbi/Desktop/MEN_2/agreement/elias-men-ratings.txt\")\n",
    "\n",
    "for line in men_raw.readlines():\n",
    "    word1s = line.split()[0]\n",
    "    word2s = line.split()[1]\n",
    "    h_r = line.split()[2]\n",
    "\n",
    "    \n",
    "print(\"Word 1 is\", line.split()[0])\n",
    "print(\"Word 2 is\", line.split()[1])\n",
    "print(\"Human relatedness is\", h_r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_io.TextIOWrapper"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(men_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ice snow [0.53915557]\n"
     ]
    }
   ],
   "source": [
    "# qui non capisco come andare oltre la riga 1...\n",
    "\n",
    "word_embedding_1 = matrix[word2idx[word1]].reshape(1, -1)\n",
    "word_embedding_2 = matrix[word2idx[word2]].reshape(1, -1)\n",
    "cos_sim = cosine_similarity(word_embedding_1, word_embedding_2)[0]\n",
    "\n",
    "print(word1, word2, cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
